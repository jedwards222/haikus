# Haiku Generator

### James Edwards, Winter 2016

## Overview

My final project is a program that generates haiku poetry. It functions using some of the framework provided for our lab 2, with methods based on the ones in those files, and some of those methods used directly. The several intermediate steps to be able to generate haikus with a program are as follows:

(1) Counting syllables in each word
(2) Creating and training a bigram model that uses a PDFSA to generate haikus
(3) Generating haikus using the bigram/PDFSA model for haikus that we created

	To achieve step (1), I copied and pasted a pronunciation dictionary from Carnegie Mellon into a plain text file. I then wrote a method to copy each word to a new text file, followed by the number of syllables in the word. This was calculated from the number of times a number (indicating a stressed or unstressed syllable) appeared following the word in the pronunciation dictionary.
	To achieve step (2), I used existing methods and made new ones to generate a dictionary object of syllable counts with words as keys, and evaluate the text of the training set as individual three line haikus.
	Finally, for step (3), the program generates the lines of the haiku individually and by adding one word at a time with a recursive function. After each word addition, the syllable count is checked, and if it is too high, the method backtracks to try a new word. If this fails a second time, the line is started over. If the word count is too low, the function adds a new word normally. When the syllable count is equal to the desired limit, the line is returned. This is performed three times, once for each line of the haiku, and then the full haiku is returned.

## Difficulties

There were several difficulties with successfully generating haikus. First of all, syllable counts can vary for words depending on how they are pronounced, and since the syllable counts were calculated from a generic pronunciation dictionary, only one of the possible counts is considered. For example, I found one poem with only 4 syllables in a line had thought that the word “spring” was two syllables (I removed the listing of spring with two syllables from the syllable dictionary after this). Another issue was that once I implemented a syllable check, it raised the likelihood of lines ending in short prepositions or articles (of, in, the, etc.) since when only one syllable remained in the line, these words were more likely to appear. Although haikus don’t require any rhyming scheme or full sentences, they often were nonsensical with endings such as these.
	One of the major bugs encountered was an infinite recursion loop when trying to backtrack if the syllable count became too high. This happened because after certain words that raised the syllable count to a high number, only words with high syllable counts were likely, so they were selected over and over again. This caused the method to repeatedly run until the compiler shut it down. I resolved this issue by only letting the program try again one time if the syllable count became too high, and then forcing it to start the line over from scratch. I used a similar approach to the issue that some words did not have any words listed as following them, which would cause the method to fail. I checked if this was the case, and if it was, forced the method to start the line over again.
	A final issue that could be analyzed more deeply is to mimic the smoothing feature of calculating probabilities for Ngrams for generating them as well. This would allow a greater variety of phrases to be produced from a limited training corpus.

## Examples

Below are some haikus generated by different training sets with some brief analysis of what selected poems could mean (all training sets were developed from haikus found [here](http://www.ahapoetry.com/aadoh/h_dictionary.htm).

### Spring Training – 1000 Lines of training Data

silence between eyes
lying in this rain and boom
the sky divided

Silence prevails momentarily in the eye of a storm, before a crash of thunder suddenly rings out and divides the sky with lightning

wild surf water view
turning green each white wave white
heavy surf tears in

tide turns leaping white
awakening furniture
moving with the sea

This haiku captures the moment when, after a low tide, the waves begin to encroach upon the beach chairs of unsuspecting visitors to the beach. Unable to move their ‘furniture’ in time, the people watch as their chairs stir and begin to move with rising sea.

### Summer Training – 4000 Lines of Training data

beach houses of boards
lovers cradled in the open
behind the valley

summer grasses watch
lovers in the beach blowing
sentinel peak stars

Failed haiku example:
rolling over the
a spider playing on the
without color of

### Autumn Training – 2000 Lines of Training data

sea fogs on the old
a window slants from earth lines
evening in darkness

tombstones marking those
ascending alone around
falling dusty white

This haiku may reference ghosts or spirits of the dead rising as the speaker walks through a graveyard. The dusty white could be ashes, snow, or the appearance of ghosts.

termite tunnel curved
autumn storms shaking moonlight
white rim rocks a spot

last swim in the north
 cactus spines above lava
leaving in a spot

### Winter Training – 2000 Lines of Training data

whale songs sung low clouds
crystal veins of rain covers
without a winter

thin dune grass weaving
half frozen snow blowing far
where whales deep a low

upward spiral wind
whales following the ocean
evening fog river

## Conclusion

The nature of haikus reduces some of the issues with Ngram generation models. Since haikus don’t follow English grammar as strictly as sentences do, a series of words that would not make sense in a sentence can function as a good haiku. The size of the training sets seemed to affect how good the haikus generated by each set sounded. The summer haikus, which had a 4000-line training set, often were entirely nonsensical, and I struggled to find any with meaning. On the other hand, the spring training set, with 1000 lines, created many better sounding haikus (I also checked the lines from these haikus and confirmed that they weren’t directly taken from the training set). The final presentation of the program generates a haiku for each of the seasons simultaneously, so there is a good chance at least one will sound like it has meaning. In conclusion, Ngram models are more suited to producing meaningful haikus than regular sentences, but in most cases this model would still be unlikely to fool readers into thinking a human had written the haikus.
